Consider the problem of wanting to figure out the numbers at N datapoints, 
given the N-1 sums of successive datapoints when we want the sum of the squared 
differences between successive datapoints to become a minimum.

Example with 3 datapoints:

index: 1     2     3
value: 10    10    30 
sums:     20    40  

10+10 = 20 (sum of datapoint 1 and 2), 10+30 = 40 (sum of datapoint 2 and 3)

the sum of squares of differences, which we shall call D, is:

D = (10-10)^2 + (30-10)^2 = 400

with the values: 5, 15, 25, we would still get the same sums (5+15=20, 15+25=40)
but smaller D = (15-5)^2 + (25-15)^2 = 200. What we want is a formula or at 
least an algorithm to find the N values v1,v2,v3,...,vN given the M=N-1 sums 
s1,s2,s3,...,sM) that solve this minimization problem.

With 5 datapoints, the problem would be

minimize:    (v2-v1)^2 + (v3-v2)^2 + (v4-v3)^2 + (v5-v4)^2 = D
constraints: v1+v2 = s1, v2+v3 = s2, v3+v4 = s3, v4+v5 = s4

we define the Lagrangian function:

L(v1,v2,..vN,l1,l2,..,lM) 
= D(v1,v2,..,vN) + something =
  (v2-v1)^2 + (v3-v2)^2 + (v4-v3)^2 + (v5-v4)^2 
+ l1*(v1+v2-s1) + l2*(v2+v3-s2) + l3*(v3+v4-s3) + l4*(v4+v5-s4)

the partial derivatives of L with respect to our data values vn and the 
Lagrange multipliers lm are:

dL/dv1 =           - 2*(v2-v1) + l1
dL/dv2 = 2*(v2-v1) - 2*(v3-v2) + l1 + l2
dL/dv3 = 2*(v3-v2) - 2*(v4-v3) + l2 + l3
dL/dv4 = 2*(v4-v3) - 2*(v5-v4) + l3 + l4
dL/dv5 = 2*(v5-v4)                  + l4
dL/dl1 = v1 + v2 - s1
dL/dl2 = v2 + v3 - s2
dL/dl3 = v3 + v4 - s3
dL/dl4 = v4 + v5 - s4

These must all be zero - let's write this as a matrix-vector equation, defining 
the vector V = (v1 l1 v2 l2 ... lM vN), i.e. we interleave the Lagrange 
multipliers lm with the original N data values vn. With this definition, the 
matrix equation to solve looks like:


 v1 l1 v2 l2 v3 l3 v4 l4 v5   ...just for orientation what the columns multiply
  
| 2  1 -2  0  0  0  0  0  0|   |v1|   |0 |
|-2  1  4  1 -2  0  0  0  0|   |l1|   |0 |
| 0  0 -2  1  4  1 -2  0  0|   |v2|   |0 |
| 0  0  0  0 -2  1  4  1 -2|   |l2|   |0 |
| 0  0  0  0  0  0 -2  1  2| * |v3| = |0 |
| 1  0  1  0  0  0  0  0  0|   |l3|   |s1|
| 0  0  1  0  1  0  0  0  0|   |v4|   |s2|
| 0  0  0  0  1  0  1  0  0|   |l4|   |s3|
| 0  0  0  0  0  0  1  0  1|   |v5|   |s4|

let's re-arrange it to a pentadiagonal system:

| 2  1 -2  0  0  0  0  0  0|   |v1|   |0 |
| 1  0  1  0  0  0  0  0  0|   |l1|   |s1|
|-2  1  4  1 -2  0  0  0  0|   |v2|   |0 |
| 0  0  1  0  1  0  0  0  0|   |l2|   |s2|
| 0  0 -2  1  4  1 -2  0  0| * |v3| = |0 |
| 0  0  0  0  1  0  1  0  0|   |l3|   |s3|
| 0  0  0  0 -2  1  4  1 -2|   |v4|   |0 |
| 0  0  0  0  0  0  1  0  1|   |l4|   |s4|
| 0  0  0  0  0  0 -2  1  2|   |v5|   |0 |

OK - the matrix looks very orderly now and is in fact symmetric. After solving 
the system with a pentadiagonal linear system solver, v1,v2,... can be located 
at every second index, starting at 0.

//-------------------------------------------------------------------------------------------------

For more flexibility, let's introduce weights w1,..,wM that multiply each 
squared difference. With these weights, we can give the squared differences of
different datapoints different amount of influence. With such weights, our 
objective functions becomes:

minimize:    w1*(v2-v1)^2 + w2*(v3-v2)^2 + w3*(v4-v3)^2 + w4*(v5-v4)^2 = D
constraints: v1+v2 = s1, v2+v3 = s2, v3+v4 = s3, v4+v5 = s4

where the constraints stay the same as above. The Lagrangian now looks like:

L(v1,v2,..vN,l1,l2,..,lM) =
  w1*(v2-v1)^2 + w2*(v3-v2)^2 + w3*(v4-v3)^2 + w4*(v5-v4)^2       ...function to minimize
+ l1*(v1+v2-s1) + l2*(v2+v3-s2) + l3*(v3+v4-s3) + l4*(v4+v5-s4)   ...constraints

the partial derivatives of L with respect to our data values vn and the 
Lagrange multipliers lm are:

dL/dv1 =              - 2*w1*(v2-v1) + l1
dL/dv2 = 2*w1*(v2-v1) - 2*w2*(v3-v2) + l1 + l2
dL/dv3 = 2*w2*(v3-v2) - 2*w3*(v4-v3) + l2 + l3
dL/dv4 = 2*w3*(v4-v3) - 2*w4*(v5-v4) + l3 + l4
dL/dv5 = 2*w4*(v5-v4)                     + l4
dL/dw1 = v1 + v2 - s1
dL/dw2 = v2 + v3 - s2
dL/dw3 = v3 + v4 - s3
dL/dw4 = v4 + v5 - s4

the matrix equation (before re-ordering and written as augmenetd cofficient matrix) 
becomes:

  v1  l1  v2       l2  v3        l3  v4       l4  v5  | rhs

| 2w1 1  -2w1      0   0         0   0        0   0   | 0  |
|-2w1 1   2(w1+w2) 1  -2w2       0   0        0   0   | 0  |
| 0   0  -2w2      1   2(w2+w3)  1  -2w3      0   0   | 0  |
| 0   0   0        0  -2w3       1   2(w3+w4) 1  -2w4 | 0  |
| 0   0   0        0   0         0  -2w4      1   2w4 | 0  |
| 1   0   1        0   0         0   0        0   0   | s1 |
| 0   0   1        0   1         0   0        0   0   | s2 |
| 0   0   0        0   1         0   1        0   0   | s3 |
| 0   0   0        0   0         0   1        0   1   | s4 |

and after re-ordering:

| 2w1 1  -2w1      0   0         0   0        0   0   | 0  |
| 1   0   1        0   0         0   0        0   0   | s1 |
|-2w1 1   2(w1+w2) 1  -2w2       0   0        0   0   | 0  |
| 0   0   1        0   1         0   0        0   0   | s2 |
| 0   0  -2w2      1   2(w2+w3)  1  -2w3      0   0   | 0  |
| 0   0   0        0   1         0   1        0   0   | s3 |
| 0   0   0        0  -2w3       1   2(w3+w4) 1  -2w4 | 0  |
| 0   0   0        0   0         0   1        0   1   | s4 |
| 0   0   0        0   0         0  -2w4      1   2w4 | 0  |

//-------------------------------------------------------------------------------------------------

Let's consider now a different cost function - one that penalizes curvature, i.e. deviations from a straight line. To see the pattern better, let's this time use 6 datapoints (and therefore 5 sums):

minimize:    (2*v2-v1-v3)^2 + (2*v3-v2-v4)^2 + (2*v4-v3-v5)^2 + (2*v5-v4-v6)^2 = D
constraints: v1+v2 = s1, v2+v3 = s2, v3+v4 = s3, v4+v5 = s4, v5+v6 = s5

concepetually, each term is the value at some datapoint minus the average of its two neighbours,
i.e. for example: v3-(v2+v4)/2, but for convenience, the whole thing is taken times two. In this 
case, our Lagrangian function becomes:

L(v1,v2,..vN,l1,l2,..,lM) = 
  (2*v2-v1-v3)^2 + (2*v3-v2-v4)^2 + (2*v4-v3-v5)^2 + (2*v5-v4-v6)^2
+ l1*(v1+v2-s1) + l2*(v2+v3-s2) + l3*(v3+v4-s3) + l4*(v4+v5-s4) + l5*(v5+v6-s5)

with partial derivatives:

dL/dv1 =                                  - 2*(2*v2-v1-v3) + l1
dL/dv2 =                   4*(2*v2-v1-v3) - 2*(2*v3-v2-v4) + l1 + l2
dL/dv3 = -2*(2*v2-v1-v3) + 4*(2*v3-v2-v4) - 2*(2*v4-v3-v5) + l2 + l3
dL/dv4 = -2*(2*v3-v2-v4) + 4*(2*v4-v3-v5) - 2*(2*v5-v4-v6) + l3 + l4
dL/dv5 = -2*(2*v4-v3-v5) + 4*(2*v5-v4-v6)                  + l4 + l5
dL/dv6 = -2*(2*v5-v4-v6)                                        + l5
dL/dl1 = v1 + v2 - s1
dL/dl2 = v2 + v3 - s2
dL/dl3 = v3 + v4 - s3
dL/dl4 = v4 + v5 - s4
dL/dl5 = v5 + v6 - s5

write the system as augmented coefficient matrix:

v1 l1 v2 l2 v3 l3 v4 l4 v5 l5 v6 | rhs
 2  1 -4  0  2  0  0  0  0  0  0 |  0
-4  1 10  1 -8  0  2  0  0  0  0 |  0
 2  0 -8  1 12  1 -8  0  2  0  0 |  0
 0  0  2  0 -8  1 12  1 -8  0  2 |  0
 0  0  0  0  2  0 -8  1 10  1 -4 |  0
 0  0  0  0  0  0  2  0 -4  1  2 |  0
 1  0  1  0  0  0  0  0  0  0  0 | s1
 0  0  1  0  1  0  0  0  0  0  0 | s2
 0  0  0  0  1  0  1  0  0  0  0 | s3
 0  0  0  0  0  0  1  0  1  0  0 | s4
 0  0  0  0  0  0  0  0  1  0  1 | s5
 
 reorder:
 
 2  1 -4  0  2  0  0  0  0  0  0 |  0
 1  0  1  0  0  0  0  0  0  0  0 | s1 
-4  1 10  1 -8  0  2  0  0  0  0 |  0
 0  0  1  0  1  0  0  0  0  0  0 | s2
 2  0 -8  1 12  1 -8  0  2  0  0 |  0
 0  0  0  0  1  0  1  0  0  0  0 | s3 
 0  0  2  0 -8  1 12  1 -8  0  2 |  0
 0  0  0  0  0  0  1  0  1  0  0 | s4 
 0  0  0  0  2  0 -8  1 10  1 -4 |  0
 0  0  0  0  0  0  0  0  1  0  1 | s5 
 0  0  0  0  0  0  2  0 -4  1  2 |  0
 
 this is a band-diagonal system with 9 nonzero diagonals.
 
 todo: do the same thing again with error weights



















//-------------------------------------------------------------------------------------------------

here's a nice article on solving pentadiagonal systems:

https://www.hindawi.com/journals/mpe/2015/232456/

Numerical Recipies has a recipie for band-digonal systems. What if 
the matrix becomes singular? Can this happen and if so, what does it mean - i 
think, there will be always a solution to this problem but could there also be 
a continuum of solutions - and if so, which one should we choose? ...maybe some 
sort of minimum-norm solution? this norm would depend on the error-weights
(see below) - we could scale all weights by the same constant and increasing
that constant would increase the relative weight of v-components with respect
to the w-components in the norm. ...but these are probably not big issues 

  

  
  
  
  
  
   
For using this for the frequency estimation problem in 
makeFreqsConsistentWithPhases, it may become slightly more complicated because
we need to take into account some extra factors...what were our sums s1,s2,.. 
here, will become 2 times the desired, new average frequencies, but maybe we 
should also take the interval lengths into account. Maybe the (v2-v1)^2 etc. 
terms should be weighted by the reciprocal of the time-difference between the 
two data points - the idea is that for data points which are far apart in time, 
a big difference between the values is more likely to occur and to be considered
less problematic (the frequency has more time to sweep up or down). i think, 
this would change all the -2 entires in the matrix to some other numbers - or 
maybe the weight should be the reciprocal of the square-root of the duration to 
cancel the square in the numerator (a linear sweep over a certain amount of 
time should get the same penalty for the same ratio of (f1-f0)/(t1-t0)), so if 
the freq changes by 100Hz in 5 sec should get the same penalty (increase of D) 
as a change by 10 Hz in 0.5s

May the Lagrange multiplier have a meaningful interpretation? May they be worth 
to keep around as additional data?  Each multiplier is a measure by how much we 
could increase/decrease the objective function by loosening the corresponding 
constraint - maybe we should look at the lambda spectrum for various signals 
and see, if we see something - could it have to do with the derivative or 
curvature or other features of the freq-trajectory? the Lagrange multiplier is
equal to the derivative of the optimum value with respect to the constraint 
constant... or something

see here at 8 min
https://www.youtube.com/watch?v=m-G3K2GPmEQ&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=98
















